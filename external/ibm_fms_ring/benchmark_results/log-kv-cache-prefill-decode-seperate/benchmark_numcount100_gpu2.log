W1120 01:54:26.620000 10152 site-packages/torch/distributed/run.py:774] 
W1120 01:54:26.620000 10152 site-packages/torch/distributed/run.py:774] *****************************************
W1120 01:54:26.620000 10152 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1120 01:54:26.620000 10152 site-packages/torch/distributed/run.py:774] *****************************************
Initializing distributed process group (Rank 1/2) with backend 'nccl'...
/venv/main/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank1]:[W1120 01:54:30.466944717 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
Initializing distributed process group (Rank 0/2) with backend 'nccl'...
/venv/main/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank0]:[W1120 01:54:31.414850114 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[INFO] Using prompt: '0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99'
[INFO] Using prompt length: 298
[INFO] Rank 1 setting up Ring Attention benchmark...
[INFO] Using model: /root/llama3
[INFO] Using tokenizer: /root/llama3
[INFO] Batch size: 1, Input Seq length: 298
[INFO] Rank 0 setting up Ring Attention benchmark...
using RingAttentionStrategy
using RingAttentionStrategy
/venv/main/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/venv/main/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once

[Benchmark] Generating and timing 30 tokens individually for 'Ring Attention' (Rank 0)...

prefill_time: 1185.3028140030801

Generated Sequence (Ring Attention):  aalborg- aalborg- aalborg- aalborg- aalborg- aalborg- aalborg- aalborg- aalborg- aalborg- aalborg- aalborg- aalborg- aalborg- aalborg- aalborg- aalborg- aalborg- aalborg- aalborg- aalborg- aalborg- aalborg- aalborg- aalborg- aalborg- aalborg- aalborg- aalborg- aalborg
Token Timings:
  * Token 1 ( aalborg): 947.98 ms
  * Token 2 ( aalborg): 696.06 ms
  * Token 3 ( aalborg): 688.71 ms
  * Token 4 ( aalborg): 690.25 ms
  * Token 5 ( aalborg): 688.89 ms
  * Token 6 ( aalborg): 689.95 ms
  * Token 7 ( aalborg): 688.97 ms
  * Token 8 ( aalborg): 689.35 ms
  * Token 9 ( aalborg): 689.15 ms
  * Token 10 ( aalborg): 689.94 ms
  * Token 11 ( aalborg): 678.61 ms
  * Token 12 ( aalborg): 674.94 ms
  * Token 13 ( aalborg): 674.77 ms
  * Token 14 ( aalborg): 676.44 ms
  * Token 15 ( aalborg): 674.99 ms
  * Token 16 ( aalborg): 675.43 ms
  * Token 17 ( aalborg): 674.70 ms
  * Token 18 ( aalborg): 674.72 ms
  * Token 19 ( aalborg): 653.27 ms
  * Token 20 ( aalborg): 651.99 ms
  * Token 21 ( aalborg): 652.17 ms
  * Token 22 ( aalborg): 651.90 ms
  * Token 23 ( aalborg): 651.96 ms
  * Token 24 ( aalborg): 651.94 ms
  * Token 25 ( aalborg): 651.93 ms
  * Token 26 ( aalborg): 652.41 ms
  * Token 27 ( aalborg): 652.05 ms
  * Token 28 ( aalborg): 652.20 ms
  * Token 29 ( aalborg): 651.95 ms
  * Token 30 ( aalborg): 651.93 ms

Summary for Ring Attention:
  Average time per token: 679.65 ms
  Median time per token: 674.86 ms
  Total generation time for 30 tokens: 20389.57 ms
[INFO] Rank 0 setting up Regular Attention benchmark...
/venv/main/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once

[Benchmark] Generating and timing 30 tokens individually for 'Regular Attention' (Rank 0)...

prefill_time: 40.62138497829437

Generated Sequence (Regular Attention): ,- -100-,- -101-,- -102-,- -103-,- -104-,- -105-,- -106-,- -107-,- -108-,- -109
Token Timings:
  * Token 1 (,): 26.96 ms
  * Token 2 ( ): 24.13 ms
  * Token 3 (100): 24.02 ms
  * Token 4 (,): 23.95 ms
  * Token 5 ( ): 23.98 ms
  * Token 6 (101): 23.93 ms
  * Token 7 (,): 23.98 ms
  * Token 8 ( ): 23.96 ms
  * Token 9 (102): 23.92 ms
  * Token 10 (,): 23.97 ms
  * Token 11 ( ): 23.89 ms
  * Token 12 (103): 24.00 ms
  * Token 13 (,): 23.98 ms
  * Token 14 ( ): 23.90 ms
  * Token 15 (104): 23.98 ms
  * Token 16 (,): 24.01 ms
  * Token 17 ( ): 23.93 ms
  * Token 18 (105): 24.00 ms
  * Token 19 (,): 24.00 ms
  * Token 20 ( ): 23.97 ms
  * Token 21 (106): 24.00 ms
  * Token 22 (,): 23.94 ms
  * Token 23 ( ): 24.00 ms
  * Token 24 (107): 24.01 ms
  * Token 25 (,): 24.01 ms
  * Token 26 ( ): 23.94 ms
  * Token 27 (108): 23.98 ms
  * Token 28 (,): 24.02 ms
  * Token 29 ( ): 23.96 ms
  * Token 30 (109): 24.00 ms

Summary for Regular Attention:
  Average time per token: 24.08 ms
  Median time per token: 23.98 ms
  Total generation time for 30 tokens: 722.33 ms
out1: shape:torch.Size([1, 1, 128256]), tensor:tensor([[[-16.7500, -11.6250, -13.5000,  ...,   5.8750,   5.8750,   5.8750]]],
       device='cuda:0', dtype=torch.bfloat16)
out2: shape:torch.Size([1, 1, 128256]), tensor:tensor([[[ 4.6250,  2.1875,  5.4688,  ..., -3.5000, -3.5000, -3.5000]]],
       device='cuda:0', dtype=torch.bfloat16)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/workspace/hetero_gpu_context_parallelism/scripts/llama_ring/benchmark_ring.py", line 307, in <module>
[rank0]:     main()
[rank0]:   File "/workspace/hetero_gpu_context_parallelism/scripts/llama_ring/benchmark_ring.py", line 303, in main
[rank0]:     torch.testing.assert_close(out1, out2)
[rank0]:   File "/venv/main/lib/python3.12/site-packages/torch/testing/_comparison.py", line 1587, in assert_close
[rank0]:     raise error_metas[0].to_error(msg)
[rank0]: AssertionError: Tensor-likes are not close!

[rank0]: Mismatched elements: 127841 / 128256 (99.7%)
[rank0]: Greatest absolute difference: 33.5 at index (0, 0, 16) (up to 1e-05 allowed)
[rank0]: Greatest relative difference: 145408.0 at index (0, 0, 28779) (up to 0.016 allowed)
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
E1120 01:55:02.821000 10152 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 10217) of binary: /venv/main/bin/python3
Traceback (most recent call last):
  File "/venv/main/bin/torchrun", line 10, in <module>
    sys.exit(main())
             ^^^^^^
  File "/venv/main/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/venv/main/lib/python3.12/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/venv/main/lib/python3.12/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/venv/main/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/main/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
scripts/llama_ring/benchmark_ring.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-11-20_01:55:02
  host      : 706257f40711
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 10217)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
