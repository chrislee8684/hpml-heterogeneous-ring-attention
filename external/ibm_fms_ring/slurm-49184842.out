Ring Attention Benchmark - Tue Nov 25 03:18:08 AM UTC 2025
Running: 256 tokens
W1125 03:18:11.655000 2637899 site-packages/torch/distributed/run.py:803] 
W1125 03:18:11.655000 2637899 site-packages/torch/distributed/run.py:803] *****************************************
W1125 03:18:11.655000 2637899 site-packages/torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1125 03:18:11.655000 2637899 site-packages/torch/distributed/run.py:803] *****************************************
Benchmark: 256 prompt tokens, 30 decode tokens
using RingAttentionStrategy
using RingAttentionStrategy

Ring:
  TTFT: 1576.04 ms | Avg Decode: 558.09 ms | Total: 18318.77 ms | Comm: 12823.92 ms | Compute/Comm: 0.43
/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  warnings.warn(  # warn only once

Regular:
  TTFT: 66.58 ms | Avg Decode: 57.10 ms | Total: 1779.55 ms

Strategy   Tokens   TTFT       Avg Decode   Total      Comm       Comp/Comm 
----------------------------------------------------------------------
Ring       256      1576.04    558.09       18318.77   12823.92   0.43      
Regular    256      66.58      57.10        1779.55    0.00       N/A       
Running: 512 tokens
W1125 03:19:01.242000 2638080 site-packages/torch/distributed/run.py:803] 
W1125 03:19:01.242000 2638080 site-packages/torch/distributed/run.py:803] *****************************************
W1125 03:19:01.242000 2638080 site-packages/torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1125 03:19:01.242000 2638080 site-packages/torch/distributed/run.py:803] *****************************************
using RingAttentionStrategy
Benchmark: 512 prompt tokens, 30 decode tokens
using RingAttentionStrategy

Ring:
  TTFT: 1513.53 ms | Avg Decode: 565.80 ms | Total: 18487.42 ms | Comm: 12773.48 ms | Compute/Comm: 0.45
/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  warnings.warn(  # warn only once

Regular:
  TTFT: 71.20 ms | Avg Decode: 55.99 ms | Total: 1751.03 ms

Strategy   Tokens   TTFT       Avg Decode   Total      Comm       Comp/Comm 
----------------------------------------------------------------------
Ring       512      1513.53    565.80       18487.42   12773.48   0.45      
Regular    512      71.20      55.99        1751.03    0.00       N/A       
Running: 1024 tokens
W1125 03:19:46.858000 2638320 site-packages/torch/distributed/run.py:803] 
W1125 03:19:46.858000 2638320 site-packages/torch/distributed/run.py:803] *****************************************
W1125 03:19:46.858000 2638320 site-packages/torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1125 03:19:46.858000 2638320 site-packages/torch/distributed/run.py:803] *****************************************
using RingAttentionStrategy
Benchmark: 1024 prompt tokens, 30 decode tokens
using RingAttentionStrategy

Ring:
  TTFT: 1667.12 ms | Avg Decode: 566.92 ms | Total: 18674.79 ms | Comm: 12690.79 ms | Compute/Comm: 0.47
/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  warnings.warn(  # warn only once

Regular:
  TTFT: 108.38 ms | Avg Decode: 56.90 ms | Total: 1815.44 ms

Strategy   Tokens   TTFT       Avg Decode   Total      Comm       Comp/Comm 
----------------------------------------------------------------------
Ring       1024     1667.12    566.92       18674.79   12690.79   0.47      
Regular    1024     108.38     56.90        1815.44    0.00       N/A       
Running: 4096 tokens
W1125 03:20:31.869000 2638551 site-packages/torch/distributed/run.py:803] 
W1125 03:20:31.869000 2638551 site-packages/torch/distributed/run.py:803] *****************************************
W1125 03:20:31.869000 2638551 site-packages/torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1125 03:20:31.869000 2638551 site-packages/torch/distributed/run.py:803] *****************************************
Benchmark: 4096 prompt tokens, 30 decode tokens
using RingAttentionStrategy
using RingAttentionStrategy

Ring:
  TTFT: 3220.51 ms | Avg Decode: 573.09 ms | Total: 20413.19 ms | Comm: 13001.37 ms | Compute/Comm: 0.57
/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  warnings.warn(  # warn only once

Regular:
  TTFT: 453.37 ms | Avg Decode: 47.90 ms | Total: 1890.37 ms

Strategy   Tokens   TTFT       Avg Decode   Total      Comm       Comp/Comm 
----------------------------------------------------------------------
Ring       4096     3220.51    573.09       20413.19   13001.37   0.57      
Regular    4096     453.37     47.90        1890.37    0.00       N/A       
Running: 8192 tokens
W1125 03:21:21.215000 2638742 site-packages/torch/distributed/run.py:803] 
W1125 03:21:21.215000 2638742 site-packages/torch/distributed/run.py:803] *****************************************
W1125 03:21:21.215000 2638742 site-packages/torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1125 03:21:21.215000 2638742 site-packages/torch/distributed/run.py:803] *****************************************
Benchmark: 8192 prompt tokens, 30 decode tokens
using RingAttentionStrategy
using RingAttentionStrategy
[rank1]:[E1125 03:31:45.913700292 ProcessGroupNCCL.cpp:683] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=97, OpType=COALESCED, NumelIn=0, NumelOut=0, Timeout(ms)=600000) ran for 600079 milliseconds before timing out.
[rank1]:[E1125 03:31:45.017471656 ProcessGroupNCCL.cpp:2241] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 97 PG status: last enqueued work: 97, last completed work: 2
[rank1]:[E1125 03:31:45.017488221 ProcessGroupNCCL.cpp:730] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E1125 03:31:45.017533482 ProcessGroupNCCL.cpp:2573] [PG ID 0 PG GUID 0(default_pg) Rank 1] First PG on this rank to signal dumping.
[rank1]:[E1125 03:31:46.614382919 ProcessGroupNCCL.cpp:1858] [PG ID 0 PG GUID 0(default_pg) Rank 1] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 97, last completed NCCL work: 2.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank1]:[E1125 03:31:46.623040205 ProcessGroupNCCL.cpp:1575] [PG ID 0 PG GUID 0(default_pg) Rank 1] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank0]: Traceback (most recent call last):
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/scripts/llama_ring_sg/benchmark_ring.py", line 227, in <module>
[rank0]:     main()
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/scripts/llama_ring_sg/benchmark_ring.py", line 191, in main
[rank0]:     result = run_benchmark(model, ids, args.num_decode_tokens, label, device, is_ring=is_ring)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/scripts/llama_ring_sg/benchmark_ring.py", line 103, in run_benchmark
[rank0]:     out = model.forward(last_token, past_key_value_states=cache, use_cache=True)
[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/models/llama.py", line 435, in forward
[rank0]:     output, cache = self._helper(
[rank0]:                     ^^^^^^^^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/models/llama.py", line 395, in _helper
[rank0]:     output = layer(
[rank0]:              ^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/models/llama_ring.py", line 27, in ring_forward
[rank0]:     attn_output = RingAttentionKernel.ring_attention(
[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/models/llama_ring.py", line 130, in ring_attention
[rank0]:     out = RingAttentionKernel._compute_attention_ring(
[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/models/llama_ring.py", line 193, in _compute_attention_ring
[rank0]:     max_score = RingAttentionKernel._max_pass(
[rank0]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/models/llama_ring.py", line 269, in _max_pass
[rank0]:     k_fp32, _ = strategy._ring_shift_tensor(k_fp32, k_len_current_block)
[rank0]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/distributed/strategy.py", line 265, in _ring_shift_tensor
[rank0]:     padded = self._pad_to_block_size(to_send, dim=seq_dim).contiguous()
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/distributed/strategy.py", line 207, in _pad_to_block_size
[rank0]:     padding = torch.zeros(*pad_shape, dtype=tensor.dtype, device=tensor.device)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: RuntimeError: zeros: Dimension size must be non-negative.
[rank1]:[E1125 03:32:45.541617983 ProcessGroupNCCL.cpp:744] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E1125 03:32:45.541682830 ProcessGroupNCCL.cpp:758] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E1125 03:32:45.551185666 ProcessGroupNCCL.cpp:2057] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=97, OpType=COALESCED, NumelIn=0, NumelOut=0, Timeout(ms)=600000) ran for 600079 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:686 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x78b9bc171b80 in /work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x247 (0x78b9bd04d5b7 in /work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x1691 (0x78b9bd0521c1 in /work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0xdf (0x78b9bd05340f in /work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd828c (0x78ba13eeb28c in /work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/bin/../lib/libstdc++.so.6)
frame #5: <unknown function> + 0x9caa4 (0x78ba1c29caa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x129c6c (0x78ba1c329c6c in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=97, OpType=COALESCED, NumelIn=0, NumelOut=0, Timeout(ms)=600000) ran for 600079 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:686 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x78b9bc171b80 in /work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x247 (0x78b9bd04d5b7 in /work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x1691 (0x78b9bd0521c1 in /work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0xdf (0x78b9bd05340f in /work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd828c (0x78ba13eeb28c in /work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/bin/../lib/libstdc++.so.6)
frame #5: <unknown function> + 0x9caa4 (0x78ba1c29caa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x129c6c (0x78ba1c329c6c in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from run at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2063 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x78b9bc171b80 in /work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe34731 (0x78b9bd029731 in /work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x9504a1 (0x78b9bcb454a1 in /work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd828c (0x78ba13eeb28c in /work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/bin/../lib/libstdc++.so.6)
frame #4: <unknown function> + 0x9caa4 (0x78ba1c29caa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x129c6c (0x78ba1c329c6c in /lib/x86_64-linux-gnu/libc.so.6)

E1125 03:32:46.357000 2638742 site-packages/torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 0 (pid: 2638770) of binary: /work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/bin/python3.11
Traceback (most recent call last):
  File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/distributed/run.py", line 936, in main
    run(args)
  File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/distributed/run.py", line 927, in run
    elastic_launch(
  File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 156, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 293, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
scripts/llama_ring_sg/benchmark_ring.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-11-25_03:32:46
  host      : umd-cscdr-gpu001.unity.rc.umass.edu
  rank      : 1 (local_rank: 1)
  exitcode  : -6 (pid: 2638772)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 2638772
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-11-25_03:32:46
  host      : umd-cscdr-gpu001.unity.rc.umass.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2638770)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Running: 16384 tokens
W1125 03:32:49.720000 2640486 site-packages/torch/distributed/run.py:803] 
W1125 03:32:49.720000 2640486 site-packages/torch/distributed/run.py:803] *****************************************
W1125 03:32:49.720000 2640486 site-packages/torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1125 03:32:49.720000 2640486 site-packages/torch/distributed/run.py:803] *****************************************
Benchmark: 16384 prompt tokens, 30 decode tokens
using RingAttentionStrategy
using RingAttentionStrategy
[rank1]:[E1125 03:43:19.489692926 ProcessGroupNCCL.cpp:683] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=97, OpType=COALESCED, NumelIn=0, NumelOut=0, Timeout(ms)=600000) ran for 600041 milliseconds before timing out.
[rank1]:[E1125 03:43:19.489971396 ProcessGroupNCCL.cpp:2241] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 97 PG status: last enqueued work: 97, last completed work: 2
[rank1]:[E1125 03:43:19.489979648 ProcessGroupNCCL.cpp:730] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E1125 03:43:19.490123633 ProcessGroupNCCL.cpp:2573] [PG ID 0 PG GUID 0(default_pg) Rank 1] First PG on this rank to signal dumping.
[rank1]:[E1125 03:43:20.276581143 ProcessGroupNCCL.cpp:1858] [PG ID 0 PG GUID 0(default_pg) Rank 1] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 97, last completed NCCL work: 2.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank1]:[E1125 03:43:20.276728951 ProcessGroupNCCL.cpp:1575] [PG ID 0 PG GUID 0(default_pg) Rank 1] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank0]: Traceback (most recent call last):
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/scripts/llama_ring_sg/benchmark_ring.py", line 227, in <module>
[rank0]:     main()
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/scripts/llama_ring_sg/benchmark_ring.py", line 191, in main
[rank0]:     result = run_benchmark(model, ids, args.num_decode_tokens, label, device, is_ring=is_ring)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/scripts/llama_ring_sg/benchmark_ring.py", line 103, in run_benchmark
[rank0]:     out = model.forward(last_token, past_key_value_states=cache, use_cache=True)
[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/models/llama.py", line 435, in forward
[rank0]:     output, cache = self._helper(
[rank0]:                     ^^^^^^^^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/models/llama.py", line 395, in _helper
[rank0]:     output = layer(
[rank0]:              ^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/models/llama_ring.py", line 27, in ring_forward
[rank0]:     attn_output = RingAttentionKernel.ring_attention(
[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/models/llama_ring.py", line 130, in ring_attention
[rank0]:     out = RingAttentionKernel._compute_attention_ring(
[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/models/llama_ring.py", line 193, in _compute_attention_ring
[rank0]:     max_score = RingAttentionKernel._max_pass(
[rank0]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/models/llama_ring.py", line 269, in _max_pass
[rank0]:     k_fp32, _ = strategy._ring_shift_tensor(k_fp32, k_len_current_block)
[rank0]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/distributed/strategy.py", line 265, in _ring_shift_tensor
[rank0]:     padded = self._pad_to_block_size(to_send, dim=seq_dim).contiguous()
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/distributed/strategy.py", line 207, in _pad_to_block_size
[rank0]:     padding = torch.zeros(*pad_shape, dtype=tensor.dtype, device=tensor.device)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: RuntimeError: zeros: Dimension size must be non-negative.
[rank1]:[E1125 03:44:20.012551759 ProcessGroupNCCL.cpp:744] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E1125 03:44:20.012571212 ProcessGroupNCCL.cpp:758] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E1125 03:44:20.013639457 ProcessGroupNCCL.cpp:2057] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=97, OpType=COALESCED, NumelIn=0, NumelOut=0, Timeout(ms)=600000) ran for 600041 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:686 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x70d321171b80 in /work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x247 (0x70d32204d5b7 in /work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x1691 (0x70d3220521c1 in /work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0xdf (0x70d32205340f in /work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd828c (0x70d378eeb28c in /work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/bin/../lib/libstdc++.so.6)
frame #5: <unknown function> + 0x9caa4 (0x70d38129caa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x129c6c (0x70d381329c6c in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=97, OpType=COALESCED, NumelIn=0, NumelOut=0, Timeout(ms)=600000) ran for 600041 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:686 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x70d321171b80 in /work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x247 (0x70d32204d5b7 in /work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x1691 (0x70d3220521c1 in /work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0xdf (0x70d32205340f in /work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd828c (0x70d378eeb28c in /work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/bin/../lib/libstdc++.so.6)
frame #5: <unknown function> + 0x9caa4 (0x70d38129caa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x129c6c (0x70d381329c6c in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from run at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2063 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x70d321171b80 in /work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe34731 (0x70d322029731 in /work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x9504a1 (0x70d321b454a1 in /work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd828c (0x70d378eeb28c in /work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/bin/../lib/libstdc++.so.6)
frame #4: <unknown function> + 0x9caa4 (0x70d38129caa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x129c6c (0x70d381329c6c in /lib/x86_64-linux-gnu/libc.so.6)

W1125 03:44:20.801000 2640486 site-packages/torch/distributed/elastic/multiprocessing/api.py:908] Sending process 2640502 closing signal SIGTERM
E1125 03:44:20.827000 2640486 site-packages/torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 0 (pid: 2640501) of binary: /work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/bin/python3.11
Traceback (most recent call last):
  File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/distributed/run.py", line 936, in main
    run(args)
  File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/distributed/run.py", line 927, in run
    elastic_launch(
  File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 156, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 293, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
scripts/llama_ring_sg/benchmark_ring.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-11-25_03:44:20
  host      : umd-cscdr-gpu001.unity.rc.umass.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2640501)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Running: 32768 tokens
W1125 03:44:24.151000 2641287 site-packages/torch/distributed/run.py:803] 
W1125 03:44:24.151000 2641287 site-packages/torch/distributed/run.py:803] *****************************************
W1125 03:44:24.151000 2641287 site-packages/torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1125 03:44:24.151000 2641287 site-packages/torch/distributed/run.py:803] *****************************************
using RingAttentionStrategy
Benchmark: 32768 prompt tokens, 30 decode tokens
using RingAttentionStrategy
[rank0]: Traceback (most recent call last):
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/scripts/llama_ring_sg/benchmark_ring.py", line 227, in <module>
[rank0]:     main()
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/scripts/llama_ring_sg/benchmark_ring.py", line 191, in main
[rank0]:     result = run_benchmark(model, ids, args.num_decode_tokens, label, device, is_ring=is_ring)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/scripts/llama_ring_sg/benchmark_ring.py", line 89, in run_benchmark
[rank0]:     out = model.forward(ids, use_cache=True)
[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/models/llama.py", line 435, in forward
[rank0]:     output, cache = self._helper(
[rank0]:                     ^^^^^^^^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/models/llama.py", line 419, in _helper
[rank0]:     gathered_dec_out = distributed_strategy.gather_tensor(dec_out, dim=1)
[rank0]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/distributed/strategy.py", line 311, in gather_tensor
[rank0]:     result = result.narrow(dim, 0, self._original_seq_len)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: RuntimeError: start (0) + length (32768) exceeds dimension size (16384).
[rank1]: Traceback (most recent call last):
[rank1]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/scripts/llama_ring_sg/benchmark_ring.py", line 227, in <module>
[rank1]:     main()
[rank1]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/scripts/llama_ring_sg/benchmark_ring.py", line 191, in main
[rank1]:     result = run_benchmark(model, ids, args.num_decode_tokens, label, device, is_ring=is_ring)
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/scripts/llama_ring_sg/benchmark_ring.py", line 89, in run_benchmark
[rank1]:     out = model.forward(ids, use_cache=True)
[rank1]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/models/llama.py", line 435, in forward
[rank1]:     output, cache = self._helper(
[rank1]:                     ^^^^^^^^^^^^^
[rank1]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/models/llama.py", line 419, in _helper
[rank1]:     gathered_dec_out = distributed_strategy.gather_tensor(dec_out, dim=1)
[rank1]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/distributed/strategy.py", line 311, in gather_tensor
[rank1]:     result = result.narrow(dim, 0, self._original_seq_len)
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]: RuntimeError: start (0) + length (32768) exceeds dimension size (16384).
E1125 03:44:55.549000 2641287 site-packages/torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 0 (pid: 2641302) of binary: /work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/bin/python3.11
Traceback (most recent call last):
  File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/distributed/run.py", line 936, in main
    run(args)
  File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/distributed/run.py", line 927, in run
    elastic_launch(
  File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 156, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 293, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
scripts/llama_ring_sg/benchmark_ring.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-11-25_03:44:55
  host      : umd-cscdr-gpu001.unity.rc.umass.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 2641303)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-11-25_03:44:55
  host      : umd-cscdr-gpu001.unity.rc.umass.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2641302)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Running: 65536 tokens
W1125 03:44:57.864000 2641372 site-packages/torch/distributed/run.py:803] 
W1125 03:44:57.864000 2641372 site-packages/torch/distributed/run.py:803] *****************************************
W1125 03:44:57.864000 2641372 site-packages/torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1125 03:44:57.864000 2641372 site-packages/torch/distributed/run.py:803] *****************************************
Benchmark: 65536 prompt tokens, 30 decode tokens
using RingAttentionStrategy
using RingAttentionStrategy
[rank0]: Traceback (most recent call last):
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/scripts/llama_ring_sg/benchmark_ring.py", line 227, in <module>
[rank0]:     main()
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/scripts/llama_ring_sg/benchmark_ring.py", line 191, in main
[rank0]:     result = run_benchmark(model, ids, args.num_decode_tokens, label, device, is_ring=is_ring)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/scripts/llama_ring_sg/benchmark_ring.py", line 89, in run_benchmark
[rank0]:     out = model.forward(ids, use_cache=True)
[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/models/llama.py", line 435, in forward
[rank0]:     output, cache = self._helper(
[rank0]:                     ^^^^^^^^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/models/llama.py", line 419, in _helper
[rank0]:     gathered_dec_out = distributed_strategy.gather_tensor(dec_out, dim=1)
[rank0]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/distributed/strategy.py", line 311, in gather_tensor
[rank0]:     result = result.narrow(dim, 0, self._original_seq_len)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: RuntimeError: start (0) + length (65536) exceeds dimension size (16384).
[rank1]: Traceback (most recent call last):
[rank1]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/scripts/llama_ring_sg/benchmark_ring.py", line 227, in <module>
[rank1]:     main()
[rank1]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/scripts/llama_ring_sg/benchmark_ring.py", line 191, in main
[rank1]:     result = run_benchmark(model, ids, args.num_decode_tokens, label, device, is_ring=is_ring)
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/scripts/llama_ring_sg/benchmark_ring.py", line 89, in run_benchmark
[rank1]:     out = model.forward(ids, use_cache=True)
[rank1]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/models/llama.py", line 435, in forward
[rank1]:     output, cache = self._helper(
[rank1]:                     ^^^^^^^^^^^^^
[rank1]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/models/llama.py", line 419, in _helper
[rank1]:     gathered_dec_out = distributed_strategy.gather_tensor(dec_out, dim=1)
[rank1]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/distributed/strategy.py", line 311, in gather_tensor
[rank1]:     result = result.narrow(dim, 0, self._original_seq_len)
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]: RuntimeError: start (0) + length (65536) exceeds dimension size (16384).
E1125 03:45:27.103000 2641372 site-packages/torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 0 (pid: 2641381) of binary: /work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/bin/python3.11
Traceback (most recent call last):
  File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/distributed/run.py", line 936, in main
    run(args)
  File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/distributed/run.py", line 927, in run
    elastic_launch(
  File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 156, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 293, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
scripts/llama_ring_sg/benchmark_ring.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-11-25_03:45:27
  host      : umd-cscdr-gpu001.unity.rc.umass.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 2641382)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-11-25_03:45:27
  host      : umd-cscdr-gpu001.unity.rc.umass.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2641381)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Running: 131072 tokens
W1125 03:45:30.494000 2641449 site-packages/torch/distributed/run.py:803] 
W1125 03:45:30.494000 2641449 site-packages/torch/distributed/run.py:803] *****************************************
W1125 03:45:30.494000 2641449 site-packages/torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1125 03:45:30.494000 2641449 site-packages/torch/distributed/run.py:803] *****************************************
using RingAttentionStrategy
Benchmark: 131072 prompt tokens, 30 decode tokens
using RingAttentionStrategy
[rank0]: Traceback (most recent call last):
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/scripts/llama_ring_sg/benchmark_ring.py", line 227, in <module>
[rank0]:     main()
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/scripts/llama_ring_sg/benchmark_ring.py", line 191, in main
[rank0]:     result = run_benchmark(model, ids, args.num_decode_tokens, label, device, is_ring=is_ring)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/scripts/llama_ring_sg/benchmark_ring.py", line 89, in run_benchmark
[rank0]:     out = model.forward(ids, use_cache=True)
[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/models/llama.py", line 435, in forward
[rank0]:     output, cache = self._helper(
[rank0]:                     ^^^^^^^^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/models/llama.py", line 419, in _helper
[rank0]:     gathered_dec_out = distributed_strategy.gather_tensor(dec_out, dim=1)
[rank0]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/distributed/strategy.py", line 311, in gather_tensor
[rank0]:     result = result.narrow(dim, 0, self._original_seq_len)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: RuntimeError: start (0) + length (131072) exceeds dimension size (16384).
[rank1]: Traceback (most recent call last):
[rank1]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/scripts/llama_ring_sg/benchmark_ring.py", line 227, in <module>
[rank1]:     main()
[rank1]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/scripts/llama_ring_sg/benchmark_ring.py", line 191, in main
[rank1]:     result = run_benchmark(model, ids, args.num_decode_tokens, label, device, is_ring=is_ring)
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/scripts/llama_ring_sg/benchmark_ring.py", line 89, in run_benchmark
[rank1]:     out = model.forward(ids, use_cache=True)
[rank1]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/models/llama.py", line 435, in forward
[rank1]:     output, cache = self._helper(
[rank1]:                     ^^^^^^^^^^^^^
[rank1]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/models/llama.py", line 419, in _helper
[rank1]:     gathered_dec_out = distributed_strategy.gather_tensor(dec_out, dim=1)
[rank1]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/hpml/hetero_gpu_inference/hpml-heterogeneous-ring-attention/external/ibm_fms_ring/fms/distributed/strategy.py", line 311, in gather_tensor
[rank1]:     result = result.narrow(dim, 0, self._original_seq_len)
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]: RuntimeError: start (0) + length (131072) exceeds dimension size (16384).
W1125 03:46:00.635000 2641449 site-packages/torch/distributed/elastic/multiprocessing/api.py:908] Sending process 2641465 closing signal SIGTERM
E1125 03:46:00.753000 2641449 site-packages/torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 0 (pid: 2641464) of binary: /work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/bin/python3.11
Traceback (most recent call last):
  File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/distributed/run.py", line 936, in main
    run(args)
  File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/distributed/run.py", line 927, in run
    elastic_launch(
  File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 156, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asadhgauri_umass_edu/.conda/envs/context_parallelism/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 293, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
scripts/llama_ring_sg/benchmark_ring.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-11-25_03:46:00
  host      : umd-cscdr-gpu001.unity.rc.umass.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2641464)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================

Results saved to: benchmark_results/summary_20251125_031808.csv
strategy,prompt_tokens,ttft_ms,avg_decode_ms,total_time_ms,comm_time_ms,compute_comm_ratio
Ring,256,1576.04,558.09,18318.77,12823.92,0.43
Regular,256,66.58,57.10,1779.55,0.00,N/A
Ring,512,1513.53,565.80,18487.42,12773.48,0.45
Regular,512,71.20,55.99,1751.03,0.00,N/A
Ring,1024,1667.12,566.92,18674.79,12690.79,0.47
Regular,1024,108.38,56.90,1815.44,0.00,N/A
Ring,4096,3220.51,573.09,20413.19,13001.37,0.57
Regular,4096,453.37,47.90,1890.37,0.00,N/A
